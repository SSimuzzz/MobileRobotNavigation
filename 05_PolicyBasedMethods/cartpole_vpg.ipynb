{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8e8ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c808c2b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class PolicyEstimator():\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        self.num_observations = n_observations\n",
    "        self.num_actions = n_actions\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.num_observations, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, self.num_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def predict(self, observation):\n",
    "        return self.network(torch.FloatTensor(observation))\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f77d5e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "### Main script ###\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "LR = 1e-2\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = PolicyEstimator(n_observations, n_actions)\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.network.parameters(), lr=LR, amsgrad=True)\n",
    "action_space = np.arange(env.action_space.n)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 15000\n",
    "else:\n",
    "    num_episodes = 15000\n",
    "\n",
    "total_rewards, batch_rewards, batch_observations, batch_actions = [], [], [], []\n",
    "batch_counter = 1\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    rewards, actions, observations = [], [], []\n",
    "    observation, info = env.reset()\n",
    "    #state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action_probs = policy_net.predict(observation).detach().numpy()\n",
    "        action = np.random.choice(action_space, p=action_probs)  # randomly select an action weighted by its probability\n",
    "\n",
    "        # push all episodic data, move to next observation\n",
    "        observations.append(observation)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if done:\n",
    "            print(i_episode, t)\n",
    "            # apply discount to rewards\n",
    "            r = np.full(len(rewards), GAMMA) ** np.arange(len(rewards)) * np.array(rewards)\n",
    "            r = r[::-1].cumsum()[::-1]\n",
    "            discounted_rewards = r - r.mean()\n",
    "\n",
    "            # collect the per-batch rewards, observations, actions\n",
    "            batch_rewards.extend(discounted_rewards)\n",
    "            batch_observations.extend(observations)\n",
    "            batch_actions.extend(actions)\n",
    "            batch_counter += 1\n",
    "            total_rewards.append(sum(rewards))\n",
    "\n",
    "            if batch_counter >= BATCH_SIZE:\n",
    "                # reset gradient\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # tensorify things\n",
    "                batch_rewards = torch.FloatTensor(batch_rewards)\n",
    "                batch_observationss = torch.FloatTensor(batch_observations)\n",
    "                batch_actions = torch.LongTensor(batch_actions)\n",
    "\n",
    "                # calculate loss\n",
    "                logprob = torch.log(policy_net.predict(batch_observations))\n",
    "                batch_actions = batch_actions.reshape(len(batch_actions), 1)\n",
    "                selected_logprobs = batch_rewards * torch.gather(logprob, 1, batch_actions).squeeze()\n",
    "                loss = -selected_logprobs.mean()\n",
    "\n",
    "                # backprop/optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # reset the batch\n",
    "                batch_rewards, batch_observations, batch_actions = [], [], []\n",
    "                batch_counter = 1\n",
    "\n",
    "            # get running average of last 100 rewards, print every 100 episodes\n",
    "            average_reward = np.mean(total_rewards[-100:])\n",
    "            if i_episode % 100 == 0:\n",
    "                print(f\"average of last 100 rewards as of episode {i_episode}: {average_reward:.2f}\")\n",
    "            #\n",
    "            # # quit early if average_reward is high enough\n",
    "            # if early_exit_reward_amount and average_reward > early_exit_reward_amount:\n",
    "            #     return total_rewards\n",
    "            #\n",
    "            # break\n",
    "\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
